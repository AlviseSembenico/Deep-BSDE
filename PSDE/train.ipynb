{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning import Trainer\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100000, 100]),\n",
       " torch.Size([10000, 100]),\n",
       " torch.Size([100000]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.load('data/train_x.pt')\n",
    "train_y = torch.load('data/train_y.pt')\n",
    "test_x  =  torch.load('data/test_x.pt')\n",
    "test_y  =  torch.load('data/test_y.pt')\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(L.LightningModule):\n",
    "    def __init__(self, partition_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(partition_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | model   | Sequential | 6.5 K  | train\n",
      "1 | loss_fn | MSELoss    | 0      | train\n",
      "-----------------------------------------------\n",
      "6.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 32/782 [00:00<00:04, 180.48it/s, v_num=4] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alvis\\.conda\\envs\\CF\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  22%|██▏       | 173/782 [00:00<00:03, 182.40it/s, v_num=4]"
     ]
    }
   ],
   "source": [
    "model = MyModel(train_x.shape[-1])\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
